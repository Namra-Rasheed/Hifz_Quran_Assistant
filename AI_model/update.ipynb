{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f73df65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\namra\\appdata\\roaming\\python\\python312\\site-packages (4.48.0)\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchaudio in c:\\programdata\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: sounddevice in c:\\programdata\\anaconda3\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: soundfile in c:\\programdata\\anaconda3\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: pyttsx3 in c:\\users\\namra\\appdata\\roaming\\python\\python312\\site-packages (2.98)\n",
      "Requirement already satisfied: gtts in c:\\users\\namra\\appdata\\roaming\\python\\python312\\site-packages (2.5.4)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\namra\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sounddevice) (1.17.1)\n",
      "Requirement already satisfied: comtypes in c:\\users\\namra\\appdata\\roaming\\python\\python312\\site-packages (from pyttsx3) (1.4.8)\n",
      "Requirement already satisfied: pypiwin32 in c:\\users\\namra\\appdata\\roaming\\python\\python312\\site-packages (from pyttsx3) (223)\n",
      "Requirement already satisfied: pywin32 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyttsx3) (305.1)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gtts) (8.1.7)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice) (2.21)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click<8.2,>=7.1->gtts) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch torchaudio sounddevice soundfile pyttsx3 gtts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31cfcbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyarabic in c:\\users\\namra\\appdata\\roaming\\python\\python312\\site-packages (0.6.15)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyarabic) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74f0b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tarteel-ai/whisper-base-ar-quran\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bb6172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import pyttsx3\n",
    "from gtts import gTTS\n",
    "import torch\n",
    "import torchaudio\n",
    "import time\n",
    "from collections import deque\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import io\n",
    "import difflib\n",
    "import scipy\n",
    "from typing import Optional\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperConfig\n",
    "from pyarabic.araby import (normalize_alef, normalize_hamza, normalize_ligature, strip_tashkeel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e29c63b-33e6-4175-a9b0-09cc7f8f0561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_surah_data_from_csv(file_path):\n",
    "    \"\"\"Read Surah data from CSV file and return as a list of dictionaries.\"\"\"\n",
    "    surah_data = []\n",
    "    try:\n",
    "        with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "            csv_reader = csv.DictReader(file)\n",
    "            for row in csv_reader:\n",
    "                surah_data.append({\n",
    "                    \"ID\": int(row[\"ID\"]),\n",
    "                    \"Name\": row[\"Name\"],\n",
    "                    \"Text\": row[\"Text\"]\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "    return surah_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff5952d0-1747-41ae-918e-900ce16195dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_text_with_colors(predicted, correct):\n",
    "    \"\"\"Highlight correct words in green and incorrect words in red.\"\"\"\n",
    "    normalized_predicted = normalize_arabic_text(predicted)\n",
    "    normalized_correct = normalize_arabic_text(correct)\n",
    "    predicted_words = predicted.split()\n",
    "    correct_words = correct.split()\n",
    "    normalized_predicted_words = normalized_predicted.split()\n",
    "    normalized_correct_words = normalized_correct.split()\n",
    "    \n",
    "    highlighted_text = []\n",
    "\n",
    "    for i in range(min(len(normalized_correct_words), len(normalized_predicted_words))):\n",
    "        if normalized_correct_words[i] == normalized_predicted_words[i]:\n",
    "            highlighted_text.append(f\"\\033[92m{predicted_words[i]}\\033[0m\")  # Green for correct\n",
    "        else:\n",
    "            highlighted_text.append(f\"\\033[91m{predicted_words[i]}\\033[0m\")  # Red for incorrect\n",
    "\n",
    "    # Handle extra words\n",
    "    if len(predicted_words) > len(correct_words):\n",
    "        for word in predicted_words[len(correct_words):]:\n",
    "            highlighted_text.append(f\"\\033[91m{word}\\033[0m\")  # Red for extra words\n",
    "\n",
    "    highlighted_text_str = ' '.join(highlighted_text)\n",
    "    return highlighted_text_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9997a3ef-6f64-4039-8052-1d10ea297af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative method to handle normalization without pyarabic.numbers\n",
    "def normalize_number(text):\n",
    "    \"\"\"A simple implementation to normalize Arabic numbers to Indian digits\"\"\"\n",
    "    arabic_numbers = \"٠١٢٣٤٥٦٧٨٩\"\n",
    "    western_numbers = \"0123456789\"\n",
    "    trans = str.maketrans(arabic_numbers, western_numbers)\n",
    "    return text.translate(trans)\n",
    "\n",
    "# Helper functions for normalization and transliteration\n",
    "arabic_to_english_transliteration = {\n",
    "    'ا': 'a', 'ب': 'b', 'ت': 't', 'ث': 'th', 'ج': 'j', 'ح': 'h', 'خ': 'kh',\n",
    "    'د': 'd', 'ذ': 'dh', 'ر': 'r', 'ز': 'z', 'س': 's', 'ش': 'sh', 'ص': 's',\n",
    "    'ض': 'd', 'ط': 't', 'ظ': 'z', 'ع': 'e', 'غ': 'gh', 'ف': 'f', 'ق': 'q',\n",
    "    'ك': 'k', 'ل': 'l', 'م': 'm', 'ن': 'n', 'ه': 'h', 'و': 'w', 'ي': 'y',\n",
    "    'أ': 'a', 'إ': 'i', 'ء': \"'\", 'ة': 'h'\n",
    "}\n",
    "\n",
    "def transliterate_arabic(text):\n",
    "    \"\"\"Transliterates Arabic text to English using a mapping.\"\"\"\n",
    "    transliterated_text = \"\".join(arabic_to_english_transliteration.get(char, char) for char in text)\n",
    "    return transliterated_text    \n",
    "\n",
    "def normalize_arabic_text(text):\n",
    "    \"\"\"Normalize Arabic text using pyarabic.\"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    text = normalize_alef(text)\n",
    "    text = normalize_hamza(text)\n",
    "    text = normalize_ligature(text)\n",
    "    text = strip_tashkeel(text)  # Remove diacritics\n",
    "    text = normalize_number(text)\n",
    "    text = re.sub(r'[\\u200b\\u200c\\u200d\\u200e\\u200f\\u202a\\u202b\\u202c\\u202d\\u202e]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70cd17e3-5ca4-49cd-8c9f-9b7baf1c2c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tts_play(text, language='ar', slow=False, volume=1.0, output_device=None):\n",
    "    \"\"\"Unified Text-to-Speech function for both Arabic and English with advanced configuration.\"\"\"\n",
    "    try:\n",
    "        if not text or not text.strip():\n",
    "            raise ValueError(\"Input text cannot be empty\")\n",
    "        if language == 'ar':\n",
    "            mp3_fp = io.BytesIO()\n",
    "            tts = gTTS(text=text, lang=language, slow=slow)\n",
    "            tts.write_to_fp(mp3_fp)\n",
    "            mp3_fp.seek(0)\n",
    "            audio_data, sample_rate = sf.read(mp3_fp)\n",
    "            audio_data = audio_data * volume\n",
    "            if output_device is not None:\n",
    "                sd.default.device = output_device\n",
    "            sd.play(audio_data, sample_rate)\n",
    "            sd.wait()\n",
    "        elif language == 'en':\n",
    "            engine = pyttsx3.init()\n",
    "            engine.say(text)\n",
    "            engine.runAndWait()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported language: {language}\")\n",
    "    except Exception as error:\n",
    "        print(f\"TTS error: {error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0388b56-6602-401b-ace7-567bc0f3d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_noise_spectral_subtraction(audio_data, sample_rate, noise_duration=0.5, smoothing_window=100):\n",
    "    \"\"\"Reduce noise using spectral subtraction.\"\"\"\n",
    "    if len(audio_data) == 0:\n",
    "        return audio_data\n",
    "    n_fft = 2048\n",
    "    hop_length = n_fft // 4\n",
    "    smoothed_audio = np.convolve(audio_data, np.ones(smoothing_window) / smoothing_window, mode='same')\n",
    "    noise_frames = int(noise_duration * sample_rate)\n",
    "    if noise_frames >= len(audio_data):\n",
    "        noise_frames = len(audio_data) // 4\n",
    "    noise_segment = smoothed_audio[:noise_frames]\n",
    "    noise_stft = np.mean(np.abs(scipy.signal.stft(noise_segment, fs=sample_rate, nperseg=n_fft, noverlap=hop_length)[2]), axis=-1)\n",
    "    freqs, time, audio_stft = scipy.signal.stft(smoothed_audio, fs=sample_rate, nperseg=n_fft, noverlap=hop_length)\n",
    "    denoised_stft = np.maximum(np.abs(audio_stft) - noise_stft[:, None], 0) * np.exp(1j * np.angle(audio_stft))\n",
    "    _, denoised_audio = scipy.signal.istft(denoised_stft, fs=sample_rate, nperseg=n_fft, noverlap=hop_length)\n",
    "    return denoised_audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b966db92-5a3a-4e1d-bca6-48c79aa2ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_similarity(word1, word2):\n",
    "    \"\"\"Calculate similarity between two words using difflib.\"\"\"\n",
    "    return difflib.SequenceMatcher(None, word1, word2).ratio()\n",
    "\n",
    "def highlight_mistakes(predicted, correct):\n",
    "    \"\"\"Comprehensively analyze differences between transcriptions with advanced similarity matching.\"\"\"\n",
    "    normalized_predicted = normalize_arabic_text(predicted)\n",
    "    normalized_correct = normalize_arabic_text(correct)\n",
    "    predicted_words = predicted.split()\n",
    "    correct_words = correct.split()\n",
    "    normalized_predicted_words = normalized_predicted.split()\n",
    "    normalized_correct_words = normalized_correct.split()\n",
    "    mistakes = []\n",
    "    matching_words = 0\n",
    "    total_words = len(correct_words)\n",
    "    for i in range(min(len(normalized_correct_words), len(normalized_predicted_words))):\n",
    "        similarity = calculate_word_similarity(normalized_correct_words[i], normalized_predicted_words[i])\n",
    "        if similarity <= 0.7:  # Threshold for considering words different\n",
    "            mistakes.append({\n",
    "                \"type\": \"replace\",\n",
    "                \"start_index\": i,\n",
    "                \"correct_words\": [correct_words[i]],  # Original word form\n",
    "                \"predicted_words\": [predicted_words[i]],  # Original word form\n",
    "                \"similarity\": similarity\n",
    "            })\n",
    "        else:\n",
    "            matching_words += 1\n",
    "    if len(predicted_words) > len(correct_words):\n",
    "        mistakes.append({\n",
    "            \"type\": \"insert\",\n",
    "            \"start_index\": len(correct_words) - 1,\n",
    "            \"correct_words\": [],\n",
    "            \"predicted_words\": predicted_words[len(correct_words):],\n",
    "            \"similarity\": 0\n",
    "        })\n",
    "    elif len(correct_words) > len(predicted_words):\n",
    "        mistakes.append({\n",
    "            \"type\": \"delete\",\n",
    "            \"start_index\": len(predicted_words) - 1,\n",
    "            \"correct_words\": correct_words[len(predicted_words):],\n",
    "            \"predicted_words\": [],\n",
    "            \"similarity\": 0\n",
    "        })\n",
    "    error_rate = (total_words - matching_words) / total_words * 100 if total_words > 0 else 0\n",
    "    accuracy_rate = (matching_words / total_words * 100) if total_words > 0 else 0\n",
    "    return {\n",
    "        \"mistakes\": mistakes,\n",
    "        \"error_rate\": error_rate,\n",
    "        \"accuracy_rate\": accuracy_rate,\n",
    "        \"total_words\": total_words,\n",
    "        \"matching_words\": matching_words\n",
    "    }\n",
    "\n",
    "def generate_mistake_guidance(mistake, correct_words):\n",
    "    \"\"\"Generate concise mistake guidance focusing only on corrections.\"\"\"\n",
    "    mistake_type = mistake['type']\n",
    "    start_index = mistake['start_index']\n",
    "    \n",
    "    # For 'replace', 'insert', and 'delete' mistakes, only focus on the correct word\n",
    "    if start_index == 0:\n",
    "        guidance_templates = {\n",
    "            'replace': f\"Correct word: '{mistake['correct_words'][0]}'\",\n",
    "            'insert': f\"Extra words: '{' '.join(mistake['predicted_words'])}'\",\n",
    "            'delete': f\"Missing word: '{mistake['correct_words'][0]}'\"\n",
    "        }\n",
    "    else:\n",
    "        # Focus only on the correct word when a mistake occurs\n",
    "        guidance_templates = {\n",
    "            'replace': f\"Correct word: '{mistake['correct_words'][0]}'\",\n",
    "            'insert': f\"Extra words: '{' '.join(mistake['predicted_words'])}'\",\n",
    "            'delete': f\"Missing word: '{mistake['correct_words'][0]}'\"\n",
    "        }\n",
    "    \n",
    "    return guidance_templates.get(mistake_type, \"Unknown error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1a34629",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuranRecitationChecker:\n",
    "    def __init__(self, checkpoint_dir=\"./quran_recitation_checkpoints\"):\n",
    "        \"\"\"\n",
    "        Initialize Quran Recitation Checker with the default base model.\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        self.load_model()  # Always load the base model\n",
    "        self.silence_duration = 2.5\n",
    "        self.sample_rate = 16000\n",
    "        self.audio_queue = deque(maxlen=int(self.silence_duration * self.sample_rate))\n",
    "        self.recording = False\n",
    "        self.audio_data = []\n",
    "        self.audio_buffer = []\n",
    "        self.debug = True\n",
    "        self.last_rms = 0\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"\n",
    "        Load the default pretrained model 'tarteel-ai/whisper-base-ar-quran'.\n",
    "        Ignores any custom model paths.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Loading the default base model: 'tarteel-ai/whisper-base-ar-quran'\")\n",
    "            config = WhisperConfig.from_pretrained(\"tarteel-ai/whisper-base-ar-quran\")\n",
    "            config.use_cache = False  # Disable caching for inference stability\n",
    "\n",
    "            # Load processor and model directly from the pretrained base model\n",
    "            self.processor = WhisperProcessor.from_pretrained(\"tarteel-ai/whisper-base-ar-quran\")\n",
    "            self.model = WhisperForConditionalGeneration.from_pretrained(\n",
    "                \"tarteel-ai/whisper-base-ar-quran\",\n",
    "                config=config\n",
    "            ).to(self.device)\n",
    "\n",
    "            self.model.eval()  # Set the model to evaluation mode\n",
    "            torch.set_grad_enabled(False)  # Disable gradients for inference\n",
    "\n",
    "            print(\"Base model loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading the base model: {e}\")\n",
    "            raise RuntimeError(\"Failed to load the base model. Check your environment and dependencies.\")\n",
    "\n",
    "    def save_audio(self, audio_data, sample_rate=16000, filename=None):\n",
    "        \"\"\"Save audio data to a file.\"\"\"\n",
    "        if filename is None:\n",
    "            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:\n",
    "                sf.write(temp_file.name, audio_data, sample_rate)\n",
    "                return temp_file.name\n",
    "        else:\n",
    "            sf.write(filename, audio_data, sample_rate)\n",
    "            return filename\n",
    "    def record_audio(self):\n",
    "        \"\"\"Record audio until consistent silence is detected or max duration is reached.\"\"\"\n",
    "        self.recording = True\n",
    "        self.audio_data = []\n",
    "        self.audio_buffer = []  # Reset buffer at start\n",
    "        silence_frames = 30  # Increased for better silence detection\n",
    "        silence_counter = 0\n",
    "        max_duration = 45  # Increased maximum duration\n",
    "        start_time = time.time()\n",
    "        print(\"Recording started... Speak clearly into the microphone.\")\n",
    "\n",
    "        def audio_callback(indata, frames, time_info, status):\n",
    "            if status:\n",
    "                print(f\"Status: {status}\")\n",
    "            \n",
    "            # Convert to mono if necessary\n",
    "            if indata.shape[1] > 1:\n",
    "                audio_chunk = indata.mean(axis=1)\n",
    "            else:\n",
    "                audio_chunk = indata.flatten()\n",
    "\n",
    "            # Store in both queue and buffer\n",
    "            self.audio_queue.extend(audio_chunk)\n",
    "            self.audio_buffer.append(audio_chunk.copy())\n",
    "            \n",
    "            # Calculate and store RMS\n",
    "            rms = np.sqrt(np.mean(audio_chunk**2))\n",
    "            self.last_rms = rms\n",
    "            \n",
    "            if self.debug:\n",
    "                print(f\"RMS: {rms:.4f} | Buffer size: {len(self.audio_buffer)} chunks\")\n",
    "\n",
    "        try:\n",
    "            # Configure and open the input stream\n",
    "            with sd.InputStream(\n",
    "                samplerate=self.sample_rate,\n",
    "                channels=1,\n",
    "                dtype=np.float32,\n",
    "                blocksize=int(self.sample_rate * 0.1),  # 100ms chunks\n",
    "                callback=audio_callback\n",
    "            ) as stream:\n",
    "                print(\"Listening... (Press Ctrl+C to stop)\")\n",
    "                \n",
    "                while self.recording and (time.time() - start_time) < max_duration:\n",
    "                    # Process the latest audio data\n",
    "                    if len(self.audio_queue) >= self.audio_queue.maxlen:\n",
    "                        rms = self.last_rms\n",
    "                        \n",
    "                        if rms < 0.005:  # Adjusted threshold\n",
    "                            silence_counter += 1\n",
    "                            if self.debug:\n",
    "                                print(f\"Silence counter: {silence_counter}/{silence_frames}\")\n",
    "                            \n",
    "                            if silence_counter >= silence_frames:\n",
    "                                print(\"Silence detected, stopping recording...\")\n",
    "                                break\n",
    "                        else:\n",
    "                            silence_counter = 0\n",
    "                    \n",
    "                    time.sleep(0.1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Recording error: {e}\")\n",
    "            return None\n",
    "        \n",
    "        finally:\n",
    "            # Combine all audio chunks\n",
    "            if self.audio_buffer:\n",
    "                combined_audio = np.concatenate(self.audio_buffer)\n",
    "                if len(combined_audio) / self.sample_rate >= 1.0:  # Minimum 1 second\n",
    "                    print(f\"Recording completed. Duration: {len(combined_audio)/self.sample_rate:.2f} seconds\")\n",
    "                    return combined_audio\n",
    "                else:\n",
    "                    print(\"Recording too short.\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(\"No audio data recorded.\")\n",
    "                return None\n",
    "            \n",
    "    def predict(self, audio_path=None, audio_data=None):\n",
    "        \"\"\"\n",
    "        Predict text from audio using Whisper model with optimized transcription.\n",
    "\n",
    "        Args:\n",
    "            audio_path (str, optional): Path to audio file\n",
    "            audio_data (numpy.ndarray, optional): Raw audio data\n",
    "\n",
    "        Returns:\n",
    "            str: Transcribed and normalized Arabic text\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not audio_path and audio_data is None:\n",
    "                raise ValueError(\"Either audio_path or audio_data must be provided\")\n",
    "\n",
    "            # Load or process audio data\n",
    "            if audio_path:\n",
    "                if self.debug:\n",
    "                    print(f\"Loading audio from file: {audio_path}\")\n",
    "                waveform, sample_rate = torchaudio.load(audio_path)\n",
    "            else:\n",
    "                if self.debug:\n",
    "                    print(\"Processing provided audio data\")\n",
    "                # Ensure audio_data is properly shaped and typed\n",
    "                audio_data = np.array(audio_data, dtype=np.float32)\n",
    "                if audio_data.ndim == 1:\n",
    "                    audio_data = audio_data.reshape(1, -1)\n",
    "                waveform = torch.from_numpy(audio_data)\n",
    "                sample_rate = self.sample_rate\n",
    "\n",
    "            # Debug information\n",
    "            if self.debug:\n",
    "                print(f\"Waveform shape: {waveform.shape}\")\n",
    "                print(f\"Sample rate: {sample_rate}\")\n",
    "                print(f\"Audio duration: {waveform.shape[-1]/sample_rate:.2f} seconds\")\n",
    "                print(f\"Max amplitude: {torch.max(torch.abs(waveform)).item():.4f}\")\n",
    "\n",
    "            # Resample if necessary\n",
    "            if sample_rate != 16000:\n",
    "                if self.debug:\n",
    "                    print(f\"Resampling from {sample_rate}Hz to 16000Hz\")\n",
    "                resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "                waveform = resampler(waveform)\n",
    "\n",
    "            # Convert to mono if necessary\n",
    "            if waveform.shape[0] > 1:\n",
    "                if self.debug:\n",
    "                    print(\"Converting stereo to mono\")\n",
    "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "            # Normalize audio if needed\n",
    "            max_amplitude = torch.max(torch.abs(waveform))\n",
    "            if max_amplitude > 1.0:\n",
    "                if self.debug:\n",
    "                    print(f\"Normalizing audio (max amplitude was {max_amplitude:.4f})\")\n",
    "                waveform = waveform / max_amplitude\n",
    "\n",
    "            # Apply noise reduction\n",
    "            waveform_np = waveform.squeeze().numpy()\n",
    "            if self.debug:\n",
    "                print(\"Applying noise reduction\")\n",
    "            denoised_audio = reduce_noise_spectral_subtraction(waveform_np, 16000)\n",
    "            denoised_waveform = torch.from_numpy(denoised_audio).unsqueeze(0)\n",
    "\n",
    "            # Prepare inputs for the model\n",
    "            if self.debug:\n",
    "                print(\"Preparing model inputs\")\n",
    "            inputs = self.processor(\n",
    "                denoised_waveform.squeeze().numpy(),\n",
    "                sampling_rate=16000,\n",
    "                return_tensors=\"pt\",\n",
    "                return_attention_mask=True,\n",
    "                padding=True\n",
    "            )\n",
    "\n",
    "            # Move inputs to appropriate device\n",
    "            input_features = inputs.input_features.to(self.device)\n",
    "            attention_mask = inputs.attention_mask.to(self.device)\n",
    "\n",
    "            # Configure generation parameters\n",
    "            generation_config = {\n",
    "                \"max_length\": 448,\n",
    "                \"num_beams\": 5,\n",
    "                \"do_sample\": True,\n",
    "                \"temperature\": 0.2,\n",
    "                \"top_k\": 50,\n",
    "                \"top_p\": 0.95,\n",
    "                \"no_repeat_ngram_size\": 2,\n",
    "                \"early_stopping\": False\n",
    "            }\n",
    "\n",
    "            # Generate prediction\n",
    "            if self.debug:\n",
    "                print(\"Generating prediction\")\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    generated_ids = self.model.generate(\n",
    "                        input_features,\n",
    "                        attention_mask=attention_mask,\n",
    "                        **generation_config\n",
    "                    )\n",
    "\n",
    "                # Decode prediction\n",
    "                predicted_text = self.processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                if self.debug:\n",
    "                    print(f\"Raw prediction: {predicted_text}\")\n",
    "\n",
    "                # Normalize the predicted text\n",
    "                normalized_text = normalize_arabic_text(predicted_text)\n",
    "\n",
    "                if self.debug:\n",
    "                    print(f\"Normalized prediction: {normalized_text}\")\n",
    "\n",
    "                return normalized_text\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    if self.debug:\n",
    "                        print(\"GPU out of memory, attempting to free cache\")\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    return self.predict(audio_path, audio_data)  # Retry once\n",
    "                raise e\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during prediction: {str(e)}\")\n",
    "            if self.debug:\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            return \"\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c902c4-b731-4997-b20c-ec5c41884731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading the default base model: 'tarteel-ai/whisper-base-ar-quran'\n",
      "Base model loaded successfully!\n",
      "Loading the default base model: 'tarteel-ai/whisper-base-ar-quran'\n",
      "Base model loaded successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 1 to read or 0 to exit:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Surahs:\n",
      "ID: 1, Name: سورة الفاتحة\n",
      "ID: 2, Name: سورة الكوثر\n",
      "ID: 3, Name: سورة الإخلاص\n",
      "ID: 4, Name: سورة الفلق\n",
      "ID: 5, Name: سورة الناس\n",
      "ID: 6, Name: سورة النصر\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution flow for Quran Recitation Checker.\"\"\"\n",
    "    try:\n",
    "        # Initialize QuranRecitationChecker (no 'model_name' argument)\n",
    "        checker = QuranRecitationChecker(checkpoint_dir=\"checkpoint-500\")\n",
    "\n",
    "        # Load the base model (custom loading removed)\n",
    "        checker.load_model()\n",
    "\n",
    "        # Ask the user whether they want to read or exit\n",
    "        user_choice = input(\"Press 1 to read or 0 to exit: \").strip()\n",
    "        if user_choice == '0':\n",
    "            print(\"Exiting the program.\")\n",
    "            return  # Exit the program\n",
    "        elif user_choice != '1':\n",
    "            print(\"Invalid input. Please press 1 to read or 0 to exit.\")\n",
    "            return\n",
    "\n",
    "        # Read Surah data from CSV file\n",
    "        csv_file_path = \"surahs.csv\"  # Replace with the actual CSV file path\n",
    "        surah_data = read_surah_data_from_csv(csv_file_path)\n",
    "        if not surah_data:\n",
    "            print(\"No Surah data found in CSV file.\")\n",
    "            return\n",
    "\n",
    "        # Display Surah IDs and Names\n",
    "        print(\"Available Surahs:\")\n",
    "        for surah in surah_data:\n",
    "            print(f\"ID: {surah['ID']}, Name: {surah['Name']}\")\n",
    "\n",
    "        # Ask user for Surah ID\n",
    "        surah_id = int(input(\"Enter the Surah ID you want to read: \").strip())\n",
    "        selected_surah = next((surah for surah in surah_data if surah[\"ID\"] == surah_id), None)\n",
    "        if not selected_surah:\n",
    "            print(\"Invalid Surah ID. Please try again.\")\n",
    "            return\n",
    "\n",
    "        correct_text = selected_surah[\"Text\"]\n",
    "        print(f\"Please read the following Surah ({selected_surah['Name']}):\")\n",
    "        print(correct_text)\n",
    "\n",
    "        # Record audio from the user\n",
    "        audio_data = checker.record_audio()\n",
    "        if audio_data is None:\n",
    "            print(\"No audio recorded, please try again.\")\n",
    "            return\n",
    "\n",
    "        # Save recorded audio to a temporary file\n",
    "        audio_file = checker.save_audio(audio_data)\n",
    "\n",
    "        # Run prediction\n",
    "        predicted_text = checker.predict(audio_path=audio_file)\n",
    "\n",
    "        mistake_analysis = highlight_mistakes(predicted_text, correct_text)\n",
    "\n",
    "        # Split the correct text into words for guidance\n",
    "        correct_words = correct_text.split()\n",
    "\n",
    "        print(\"\\n--- Recitation Analysis ---\")\n",
    "        highlighted_text = highlight_text_with_colors(predicted_text, correct_text)\n",
    "        print(f\"Your Recitation: {highlighted_text}\")\n",
    "        print(f\"Expected Recitation: {correct_text}\")\n",
    "\n",
    "        if mistake_analysis['mistakes']:\n",
    "            for mistake in mistake_analysis['mistakes']:\n",
    "                guidance = generate_mistake_guidance(mistake, correct_words)\n",
    "                print(guidance)\n",
    "                tts_play(guidance, language='ar')  # TTS for mistake guidance\n",
    "        else:\n",
    "            print(\"Excellent recitation!\")\n",
    "            tts_play(\"Excellent recitation!\", language='ar')\n",
    "\n",
    "        print(f\"Accuracy Rate: {mistake_analysis['accuracy_rate']}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7dec23-76d7-47db-b68f-b5d96330cd70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
